#!/usr/bin/env python# -*- coding:UTF-8 -*-'''@Author: Li Fajin@Date: 2019-08-12 17:52:34@LastEditors: Li Fajin@LastEditTime: 2019-08-19 20:26:25@Description:	Containing some common functions used for other scripts:	1. bam_file_attr(): a class used for define attribution of bam files.	2. parse_gtfFile(): a function used for parsing gtf file.	3. parse_coorFile(): a function used for parsing transcript_cds.txt file generated by RiboCode.	4. get_trans_length_dict(): a function used for getting a length dict of all transcripts based on transcripts_sequence.fa generated by RiboCode.	5. get_longest_transcripts_information(): a function used for getting longest transcript information of protein coding genes.	6. get_all_transcripts_information(): a function used for getting all transcript information.	7. reload_transcripts_information(): a function used for re-get longest transcript information based on file generated by get_longest_transcripts_information().	8. getWindowsVector(): a function used for lining up all transcripts density vectors when do the metagene analysis.	9. lengths_offsets_split(): a function used for getting read length and offset separated by comma.	10. get_trans_frame_counts(): a function used for counts calculation at each position along transcripts.	11. RPKM_of_all_genes(): a function used for RPKM calculation based on reads covered on the whole transcripts or on CDS region.	12. write_bam_file_read_counts_dataframe(): function used for output RPKM values.	13. write_bam_file_density_dataframe(): a function used for outputing density dataframe if not set --CI parameter.	14. parse_bamListFile(): a function used for parsing bam attribute files constructured by user themselves.	15. fasta_attrbution(): a class used for define attribution of fasta files.	16. fastaIter(): a function used for getting a sequence dict based on a fasta file.	17. translation(): a function used for translating a DNA seq to AA seq.	18. flatten(): a function used for flatten all iterable objects.'''from __future__ import divisionfrom __future__ import absolute_importfrom __future__ import print_functionimport sysimport osimport pysamimport itertoolsfrom itertools import groupbyimport numpy as npimport pandas as pdfrom Bio.Seq import translateclass bam_file_attr(object):	"""Class for bam file attribute"""	def __init__(self,bamName,bamLen,bamOffset,bamLegend):		self.bamName=bamName		self.bamLen=bamLen		self.bamOffset=bamOffset		self.bamLegend=bamLegendclass fasta_attrbution(object):	"""Class for fasta file attribute"""	def __init__(self,fastaName,fastaLegend):		self.fastaName=fastaName		self.fastaLegend=fastaLegenddef parse_gtfFile(gtfFile):		'''extract trancript information from standard GTF file'''		transID2GeneIDDict={}		geneIDNameDict={}		proteinCodingGeneDict={}		transWithStrand={}		transID_start_coor={}		transID_stop_coor={}		with open(gtfFile,'r') as f:			for line in f:				if line.strip()[0] == "#" or (not line.strip()):					continue				fields=line.strip().split("\t")				strand=fields[6]				trans_start=str(fields[3])				trans_stop=str(fields[4])				if len(fields) < 9:					raise KeyError("Sorry, you get a unregular GTF file. Please check it again.")				KeyDesc={i.strip().split(" ")[0]:i.strip().split(" ")[1].strip('"') for i in fields[8].strip(';').split(';')}				if fields[2]=='transcript' and ('transcript_biotype "protein_coding"' in line.strip() or 'transcript_type "protein_coding"' in line.strip()) :					geneID=KeyDesc['gene_id']					transID=KeyDesc['transcript_id']					transID2GeneIDDict[transID]=geneID					transWithStrand[transID]=strand					transID_start_coor[transID]=trans_start					transID_stop_coor[transID]=trans_stop					if "gene_name" not in KeyDesc.keys():						geneName=geneID					else:						geneName=KeyDesc['gene_name']					geneIDNameDict[geneID]=geneName					if geneID not in proteinCodingGeneDict:						proteinCodingGeneDict[geneID]=set([transID])					else:						proteinCodingGeneDict[geneID].add(transID)		return transID2GeneIDDict,geneIDNameDict,proteinCodingGeneDict,transWithStrand,transID_start_coor,transID_stop_coordef parse_coorFile(coorFile):		'''extract transcript information from coorFile generated by RiboCode'''		startCodonCoorDict={}		stopCodonCoorDict={}		with open(coorFile,'r') as f:			for coors in f:				startCodonCoorDict[coors.strip().split('\t')[0]]=coors.strip().split('\t')[1]				stopCodonCoorDict[coors.strip().split('\t')[0]]=coors.strip().split('\t')[2]		return startCodonCoorDict,stopCodonCoorDictdef get_trans_length_dict(transcriptFile):	'''	This function is used to get a dict of transcript length	'''	trans_length_dict={}	transFile=open(transcriptFile,'r')	faiter=(x[1] for x in groupby(transFile,lambda line: line.strip()[0]==">")) ## groupby returns a tuple (key, group)	for header in faiter:		trans_id=header.__next__().strip(">").split(" ")[0]		seq=''.join(s.strip() for s in faiter.__next__())		length=int(len(seq.strip()))		trans_length_dict[trans_id]=length	return trans_length_dictdef get_longest_transcripts_information(coorFile,transcriptFile,gtfFile,longestTransFile):	'''	This function is aimed to get four kinds of information:	1) all transcripts id of the longest isoform of all protein coding genes containing both start codon and stop codon: selectTrans	2) start codon dictionary of all transcripts selected above: startCodonCoorDict	3) stop codon dictionary of all transcripts selected above: stopCodonCoorDict	4) transcript length dictionary of all transcripts selected above: transLengthDict	'''	transID2GeneIDDict,geneIDNameDict,proteinCodingGeneDict,transWithStrand,transID_start_coor,transID_stop_coor=parse_gtfFile(gtfFile)	startCodonCoorDict,stopCodonCoorDict=parse_coorFile(coorFile)	## select the longest transcript for each gene	transLengthDict=get_trans_length_dict(transcriptFile)	selectLongestTrans=set()	for trans_id in proteinCodingGeneDict.values():		if len(trans_id) >1:			tmp = sorted(list(trans_id),key=lambda x: transLengthDict[x])[-1]			selectLongestTrans.add(tmp)		else:			selectLongestTrans.add(list(trans_id)[0])	## select transcripts with both start codon and stop codon	transWithBothStartAndStopCodon=set(startCodonCoorDict.keys()).intersection(set(stopCodonCoorDict.keys()))	selectTrans=transWithBothStartAndStopCodon.intersection(set(transLengthDict.keys()))	selectTrans=selectTrans.intersection(selectLongestTrans)	print(str(len(selectTrans))+'  transcripts will be used in the follow analysis.\n', file=sys.stderr)	## output the longest transcript of each gene	with open(longestTransFile,'w') as fout:		fout.write("%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n" %("trans_id","strand","gene_id","gene_name","gene_start","gene_stop","trans_start","trans_start","CDS_length","5UTR_length","3UTR_length","transcript_length"))		for ltid in selectLongestTrans:			geneID=transID2GeneIDDict[ltid]			cds_length=int(stopCodonCoorDict[ltid])-int(startCodonCoorDict[ltid])+1			Five_UTR_length=int(startCodonCoorDict[ltid])-1			Three_UTR_length=int(transLengthDict[ltid])-int(stopCodonCoorDict[ltid])			fout.write("%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n" % (ltid,transWithStrand[ltid],geneID,geneIDNameDict[geneID],transID_start_coor[ltid],transID_stop_coor[ltid],startCodonCoorDict[ltid],stopCodonCoorDict[ltid],cds_length,str(Five_UTR_length),str(Three_UTR_length),str(transLengthDict[ltid])))def get_all_transcripts_information(coorFile,transcriptFile,gtfFile,allTranscriptsInfo):	with open(gtfFile) as fin:		transID_geneID_Dict={}		geneID_Name_Dict={}		geneID_Biotype_Dict={}		transID_strand_Dict={}		transID_start_coor={}		transID_stop_coor={}		for line in fin:			if line[0] == "#" or (not line.strip()):				continue			fields=line.strip().split("\t")			strand=strand=fields[6]			trans_start=str(fields[3])			trans_stop=str(fields[4])			if len(fields) < 9:					raise KeyError("Sorry, you get a unregular GTF file. Please check it again.")			KeyDesc={i.strip().split(" ")[0]:i.strip().split(" ")[1].strip('"') for i in fields[8].strip(';').split(';')}			if line.strip().split('\t')[2]=='transcript' :				transID=KeyDesc['transcript_id']				geneID=KeyDesc['gene_id']				transID_geneID_Dict[transID]=geneID				transID_strand_Dict[transID]=strand				transID_start_coor[transID]=trans_start				transID_stop_coor[transID]=trans_stop				if "gene_name" not in KeyDesc.keys():					#some genes dont have gene_name but everyone has the gene_id					print(geneID)					geneName=geneID					biotype=KeyDesc['transcript_biotype']				else :					geneName=KeyDesc['gene_name']					biotype=KeyDesc['transcript_biotype']				geneID_Name_Dict[geneID]=geneName				geneID_Biotype_Dict[geneID]=biotype	###get start_codon stop_codon transcript coordinate	startCodonCoorDict,stopCodonCoorDict=parse_coorFile(coorFile)	## get transcript length	transLengthDict=get_trans_length_dict(transcriptFile)	#####write Dict to outfile	with open(allTranscriptsInfo,"w") as fout:		fout.write("%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n" % ("trans_id","strand","gene_id","gene_name","gene_biotype","gene_start","gene_stop","trans_start","trans_stop","CDS_length","5UTR_length","3UTR_length","transcript_length"))		for fn in sorted( startCodonCoorDict.keys() ):			geneID=transID_geneID_Dict[fn]			cds=int(stopCodonCoorDict[fn])-int(startCodonCoorDict[fn])+1			Five_UTR=int(startCodonCoorDict[fn])-1			Three_UTR=int(transLengthDict[fn])-int(stopCodonCoorDict[fn]) ## length-stop+1-3			fout.write("%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n" % (fn,transID_strand_Dict[fn],geneID,geneID_Name_Dict[geneID],geneID_Biotype_Dict[geneID],str(transID_start_coor[fn]),str(transID_stop_coor[fn]),str(startCodonCoorDict[fn]),str(stopCodonCoorDict[fn]),str(cds),str(Five_UTR),str(Three_UTR),str(transLengthDict[fn])))def reload_transcripts_information(longestTransFile):	selectTrans=set()	transLengthDict={}	cdsLengthDict={}	startCodonCoorDict={}	stopCodonCoorDict={}	transID2geneID={}	transID2geneName={}	with open(longestTransFile,'r') as f:		for line in f:			if line.strip()=='':				continue			if line.strip().split("\t")[0] == 'trans_id':				continue			transID=line.strip().split("\t")[0]			geneID=line.strip().split("\t")[2]			geneName=line.strip().split("\t")[3]			startCodon=int(line.strip().split("\t")[6])			stopCodon=int(line.strip().split("\t")[7])			cds_length=int(line.strip().split("\t")[8])			transLength=int(line.strip().split("\t")[11])			selectTrans.add(transID)			transLengthDict[transID]=transLength			startCodonCoorDict[transID]=startCodon			stopCodonCoorDict[transID]=stopCodon			transID2geneID[transID]=geneID			transID2geneName[transID]=geneName			cdsLengthDict[transID]=cds_length			# print(transID,geneID,geneName,startCodon,stopCodon,transLength)	print(str(len(selectTrans))+'  transcripts will be used in the follow analysis.\n', file=sys.stderr)	return selectTrans,transLengthDict,startCodonCoorDict,stopCodonCoorDict,transID2geneID,transID2geneName,cdsLengthDictdef getWindowsVector(upLength,downLength,transVector,inCoor):		"""		get every transcript windows reads density vector		"""		## inCoor is 0-based coordinate		windowsVector=np.zeros(int(upLength+downLength+1),dtype="float64")		posVector=np.zeros(int(upLength+downLength+1),dtype="int64")		## left		if inCoor <= upLength :				transLeftIndex=0				windowsLeftIndex=upLength-inCoor		else:				transLeftIndex=inCoor-upLength				windowsLeftIndex=0		## right		if len(transVector)-(inCoor+1) <= downLength:				transRightIndex=len(transVector) ## python splice do not contain the last one position,so the last index should be len(transVector)-1+1				windowsRightIndex=upLength-inCoor+len(transVector)		else:				transRightIndex=inCoor+1+downLength				windowsRightIndex=len(windowsVector)		## final vector for that transcript		windowsVector[windowsLeftIndex:windowsRightIndex]+=transVector[transLeftIndex:transRightIndex]		posVector[windowsLeftIndex:windowsRightIndex]+=1		return (windowsVector,posVector)def lengths_offsets_split(value):		''' Split the given comma separated values to multiple integer values'''		values=[]		for item in value.split(','):				item=int(item)				values.append(item)		return valuesdef get_trans_frame_counts(ribo_fileobj, transcript_name, read_lengths, read_offsets, transLength, startCoor, stopCoor):	"""For each mapped read of the given transcript in the BAM file,get the P-site and codon unit reads density	ribo_fileobj -- file object - BAM file opened using pysam AlignmentFile	transcript_name -- Name of transcript to get counts for	read_length -- If provided, get counts only for reads of this length.	read_offsets -- the offset length corresponding to 5' mapped position.	transLength -- the length of the transcript.	startCoor -- the coordinate of the first base of start codon 0-based.	stopCoor -- the coordinate of the first base of stop codon 0-based.	"""	read_counts = np.zeros(transLength,dtype="int64")	total_reads = 0	if read_lengths == "ALL" : ## RNA		for record in ribo_fileobj.fetch(transcript_name):			if record.flag == 16 or record.flag == 272:				continue			total_reads += 1			position = record.pos			read_counts[position]+=1	else:		read_lengths=lengths_offsets_split(read_lengths)		read_offsets=lengths_offsets_split(read_offsets)		for record in ribo_fileobj.fetch(transcript_name):			if record.flag == 16 or record.flag == 272:				continue			for R_length, R_offset in zip(read_lengths,read_offsets):				if  record.query_length == R_length :					# if an offset is specified, increment position by that offset.					position = record.pos + R_offset ## transform into the position of P-site				else:					# ignore other reads/lengths					continue				total_reads += 1				try:					read_counts[position]+=1				except KeyError:					print("Dont has this position after offset : transcript_name -> position"+" "+transcript_name+" -> "+position)	#get trans counts for each 3 frames	read_counts_frame0=read_counts[(startCoor+0):(stopCoor-2):3]	read_counts_frame1=read_counts[(startCoor+1):(stopCoor-1):3]	read_counts_frame2=read_counts[(startCoor+2):(stopCoor-0):3]	read_counts_frameSum=read_counts_frame0+read_counts_frame1+read_counts_frame2	cds_reads=sum(read_counts_frameSum)	return read_counts,read_counts_frameSum,total_reads,cds_readsdef RPKM_of_all_genes(in_bamFile,in_selectTrans,in_transLengthDict,in_startCodonCoorDict,in_stopCodonCoorDict,in_readLengths,in_readOffset,Type):	'''calculate the RPKM values for the CDS region or all transcript region'''	pysamFile=pysam.AlignmentFile(in_bamFile,'rb')	# in_selectTrans=list(in_selectTrans)	RPKM={}	all_counts=0	for trans in in_selectTrans:		leftCoor =int(in_startCodonCoorDict[trans])-1		rightCoor=int(in_stopCodonCoorDict[trans])-3		(trans_counts,read_counts_frameSum,total_reads,cds_reads)=get_trans_frame_counts(pysamFile, trans, in_readLengths, in_readOffset, in_transLengthDict[trans], leftCoor, rightCoor)		if Type.upper() in ['CDS']:			all_counts+=cds_reads		elif Type.upper() in ['TRANSCRIPT','TRANS','TRANSCRIPTS']:			all_counts+=total_reads		else:			raise IOError("please choose your type of RPKM.[CDS or exon]")	for trans in in_selectTrans:		leftCoor =int(in_startCodonCoorDict[trans])-1		rightCoor=int(in_stopCodonCoorDict[trans])-3		(trans_counts,read_counts_frameSum,total_reads,cds_reads)=get_trans_frame_counts(pysamFile, trans, in_readLengths, in_readOffset, in_transLengthDict[trans], leftCoor, rightCoor)		if Type.upper() in ['CDS']:			cds_reads_normed=10**9*(cds_reads/(all_counts*len(read_counts_frameSum)))			RPKM[trans]=cds_reads_normed		elif Type.upper() in ['TRANSCRIPT','TRANS','TRANSCRIPTS','EXON','EXONS']:			trans_counts_normed=10**9*(total_reads/(all_counts*len(trans_counts)))			RPKM[trans]=trans_counts_normed	return RPKMdef write_bam_file_density_dataframe(inBamAttr,outFile):	data=[]	for bms in inBamAttr:		k=pd.DataFrame([bms.bamLegend]*len(bms.start_density))		start=pd.DataFrame(bms.start_density)		stop=pd.DataFrame(bms.stop_density)		density=pd.merge(start,stop,how="left",left_index=True,right_index=True)		density=pd.merge(k,density,how="left",left_index=True,right_index=True)		data.append(density)	temp=data[0]	if len(data) < 1:		raise EOFError("Empty file, there is nothing in the file.")	if len(data) == 1:		temp.columns=['sample','start_density','stop_density']		temp.to_csv(outFile,sep="\t",index=0)	else:		for i in np.arange(1,len(data)):			temp=np.vstack((temp,data[i]))		temp=pd.DataFrame(temp,columns=["sample","start_density","stop_density"])		temp.to_csv(outFile,sep="\t",index=0)def write_bam_file_read_counts_dataframe(inBamAttr,outFile):	data=[]	data_index=[]	for bms in inBamAttr:		d=bms.RPKM		i=bms.bamLegend		data.append(d)		data_index.append(i)	data=pd.DataFrame(data,index=data_index)	data=data.T	data.to_csv(outFile,sep="\t")def parse_bamListFile(bamListFile):	bamFileList=[]	readLengthsList=[]	OffsetsList=[]	bamLegendsList=[]	flag=1	with open(bamListFile,'r') as f:		for line in f:			if flag == 1:				flag+=1				continue			bamFile=line.strip().split("\t")[0]			readLengths=line.strip().split("\t")[1]			Offsets=line.strip().split("\t")[2]			bamLegends=line.strip().split("\t")[3]			bamFileList.append(bamFile)			readLengthsList.append(readLengths)			OffsetsList.append(Offsets)			bamLegendsList.append(bamLegends)	return bamFileList,readLengthsList,OffsetsList,bamLegendsListdef fastaIter(transcriptFile):	'''	This function is used to get a dict of transcript sequence	'''	fastaDict={}	f=open(transcriptFile,'r')	faiter=(x[1] for x in groupby(f,lambda line: line.strip()[0]==">")) ## groupby returns a tuple (key, group)	for header in faiter:		geneName=header.__next__().strip(">").split(" ")[0]		seq=''.join(s.strip() for s in faiter.__next__())		fastaDict[geneName]=seq	return fastaDictdef translation(seq,table=1,cds=True):	"""	translate the DNA to protein sequence using the translation table	table = 1, is the standard table, ref: https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi	Code from RiboCode.[Xiao,et al. NAR. 2018]	"""	if len(seq) % 3 != 0:		sys.stderr.write("Warning: sequence is not divisible by 3\n")		seq = seq[:-(len(seq) % 3)]	return translate(seq,table=table,cds=cds)def flatten(xs):	for x in xs:		if isinstance(x,tuple):			for xx in flatten(x):				yield xx		else:			yield x